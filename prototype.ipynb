{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General overview\n",
    "```\n",
    "Europarl (files) --// My code //--> files:\n",
    "    Bi-Text (SRC ||| TRG)\n",
    "    SRC\n",
    "    TRG\n",
    "    GRAMMAR (TRG)\n",
    "    IDS\n",
    "\n",
    "SRC --// My code //--> SRC_numbered (or \"SRC_forFSL\")\n",
    "\n",
    "Bi-Text --// Awsome //--> ALIGNMENTS\n",
    "\n",
    "SRC_numbered --// Open Sesame //--> FSL_numbered.conll\n",
    "\n",
    "FSL_numbered --// My code //--> New SRC            (realigned)\n",
    "TRG                             New TRG    \n",
    "GRAMMAR                         New GRAMMAR\n",
    "IDS                             New IDS\n",
    "ALIGNMENTS                      New ALIGNMENTS\n",
    "\n",
    "FSL_numbered --// My code //--> PP_FSL_numbered (simplified)\n",
    "\n",
    "PP_FSL  --// My code //--> Frame semantic transfer\n",
    "New ALIGNMENTS\n",
    "New GRAMMAR\n",
    "--------------\n",
    "New SRC\n",
    "New TRG\n",
    "(New IDS)\n",
    "\n",
    "Frame semantic transfer --> prefered format, xml?\n",
    "\n",
    "```  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def id2score(string):\n",
    "    \"\"\" Takes an id of the form 'ep-00-02-15.2261' and returns a score. \n",
    "    \"\"\"\n",
    "    first_split = string.split(\".\")\n",
    "    number = first_split[-1]\n",
    "    second_split = first_split[0].split(\"-\")\n",
    "    date = tuple(second_split[1:])\n",
    "    return date, number\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractor(src_file, trg_file, bitxt, src_out, trg_out, trg_dep, ids, stop = 1000, add_id = False):\n",
    "\n",
    "    src_parser = ET.XMLPullParser(['start', 'end'])\n",
    "    trg_parser = ET.XMLPullParser(['start', 'end'])\n",
    "\n",
    "    src_parser.feed(\"<root>\") # we need a root-token; https://stackoverflow.com/questions/38853644/python-xml-parseerror-junk-after-document-element\n",
    "    trg_parser.feed(\"<root>\")\n",
    "\n",
    "    line = None\n",
    "    sub_loop = None\n",
    "    sub_id = None\n",
    "    skip_sub = None\n",
    "\n",
    "    #sentence_pairs = []\n",
    "    \n",
    "    if stop != None:\n",
    "        STOP = stop\n",
    "        i = 0\n",
    "        \n",
    "    with open(src_file, \"r\") as srcf, open(trg_file, \"r\") as trgf, open(bitxt, \"w\") as bit_f, open(src_out, \"w\") as src_f, open(trg_out, \"w\") as trg_f, open(trg_dep, \"w\") as trgdep_f, open(ids, \"w\") as ids_f:\n",
    "\n",
    "        while line != \"</corpus>\":\n",
    "            line = srcf.readline()\n",
    "            line = line.strip(\"\\n\")\n",
    "\n",
    "            if line[:11] == \"<linken id=\":\n",
    "                welcome = True\n",
    "                sent_id = line[12:-2]\n",
    "                src_segment = [sent_id] if add_id else []\n",
    "                identity = sent_id\n",
    "                continue\n",
    "            if line[:3] == \"<w \" and welcome == True:\n",
    "                src_parser.feed(line)\n",
    "                as_list = [x for x in src_parser.read_events()]\n",
    "                event, element = as_list[-1]\n",
    "                src_segment.append(element.text)\n",
    "                continue\n",
    "            if line == \"</linken>\":\n",
    "                welcome = False\n",
    "                sub_loop = None\n",
    "\n",
    "                if sent_id == sub_id:\n",
    "                    loop_hole = True\n",
    "                    trg_segment = [sub_id] if add_id else []\n",
    "                    grammar = []\n",
    "\n",
    "                while sub_loop != \"</linken>\":\n",
    "\n",
    "                    sub_loop = trgf.readline()\n",
    "                    sub_loop = sub_loop.strip(\"\\n\")\n",
    "                    if sub_loop[:11] == \"<linken id=\":\n",
    "                        sub_id = sub_loop[12:-2]\n",
    "                        if sent_id == sub_id:\n",
    "                            loop_hole = True\n",
    "                            trg_segment = [sub_id] if add_id else []\n",
    "                            grammar = []\n",
    "                            continue\n",
    "                        else:\n",
    "                            date, score = id2score(sent_id)\n",
    "                            sub_date, sub_score = id2score(sub_id)\n",
    "                            #print(\"===> IDs\",sent_id, \"(EN)\", sub_id, \"(SV)\")\n",
    "                            #if sent_id.split(\".\")[-1] < sub_id.split(\".\")[-1]:\n",
    "                            if date == sub_date and score < sub_score:\n",
    "                                src_segment = [] if add_id else [sent_id]\n",
    "                                break\n",
    "                            else:\n",
    "                                skip_sub = True\n",
    "                                continue\n",
    "\n",
    "                    if sub_loop[:3] == \"<w \" and loop_hole == True:\n",
    "                        trg_parser.feed(sub_loop)\n",
    "                        s_list = [x for x in trg_parser.read_events()]\n",
    "                        sub_event, sub_element = s_list[-1]\n",
    "                        trg_segment.append(sub_element.text)\n",
    "                        g = sub_element.attrib\n",
    "                        syntax = [g[\"ref\"], g[\"dephead\"], g[\"deprel\"]]\n",
    "                        grammar.append(\";\".join(syntax))\n",
    "                        continue\n",
    "                    \n",
    "                    if sub_loop == \"</linken>\":\n",
    "                        if skip_sub == True:\n",
    "                            skip_sub = False\n",
    "                            sub_loop = None\n",
    "                            continue\n",
    "                        loop_hole = False\n",
    "                        \n",
    "                        if stop == None:\n",
    "                        \n",
    "                            bit_f.write(\" \".join(src_segment) + \" ||| \" + \" \".join(trg_segment) + \"\\n\") \n",
    "                            src_f.write(\" \".join(src_segment) + \"\\n\") \n",
    "                            trg_f.write(\" \".join(trg_segment) + \"\\n\") \n",
    "                            trgdep_f.write(\" \".join(grammar) + \"\\n\") \n",
    "                            ids_f.write(identity + \"\\n\")\n",
    "                        \n",
    "                        else:\n",
    "                            if i in [n for n in range(0, STOP, 5)]:\n",
    "                                bit_f.write(\" \".join(src_segment) + \" ||| \" + \" \".join(trg_segment) + \"\\n\") \n",
    "                                src_f.write(\" \".join(src_segment) + \"\\n\") \n",
    "                                trg_f.write(\" \".join(trg_segment) + \"\\n\") \n",
    "                                trgdep_f.write(\" \".join(grammar) + \"\\n\")\n",
    "                                ids_f.write(identity + \"\\n\")\n",
    "                                \n",
    "                            i += 1\n",
    "                        \n",
    "                        src_segment = []\n",
    "                        trg_segment = []\n",
    "                        grammar = []\n",
    "            \n",
    "            if stop != None:\n",
    "                if i > STOP:\n",
    "                    break\n",
    "\n",
    "    src_parser.feed(\"</root>\") # perhaps not really required, but true to standard ...\n",
    "    trg_parser.feed(\"</root>\")      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUT\n",
    "path     =  \"/home/max/corpora/europarl/\"\n",
    "en_file  =  path + \"europarl-en.xml\"\n",
    "sv_file  =  path + \"europarl-sv.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OUTPUT\n",
    "scratch = True\n",
    "out_dir = \"../data/\"\n",
    "bitxt   = out_dir + \"bitext_scratch.txt\" if scratch else out_dir + \"bitext.txt\"\n",
    "src_out = out_dir + \"src_scratch.txt\" if scratch else out_dir + \"src.txt\"\n",
    "trg_out = out_dir + \"trg_scratch.txt\" if scratch else out_dir + \"trg.txt\"\n",
    "trg_dep = out_dir + \"trg_dep_scratch.txt\" if scratch else out_dir + \"trg_dep.txt\" #what about json here???\n",
    "ids     = out_dir + \"ids_scratch.txt\" if scratch else out_dir + \"ids.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extractor(en_file, sv_file, bitxt, src_out, trg_out, trg_dep, ids, stop=10000, add_id=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumerate for FSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_in = src_out\n",
    "file_out = src_out[:-4] + \"_forFSL.txt\"\n",
    "print(file_in, \">>>\", file_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_in, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "with open(file_out, \"w\") as f:\n",
    "    for i, line in enumerate(lines, start = 1):\n",
    "        f.write(f\"#{i} {line}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Postprocessing of FSL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagnostics: Output from sesame vs. what went in\n",
    "The number of sentences that are feed to Open Sesame is note the same number that goes in. IN > OUT. This messes up the synchronization of data in the pipline. \n",
    "\n",
    "The most obvious solution is to number sentences. However note that this affects the FSL as the numbers are in some case assigned FEs. These annotations can be ignored, but then note special cases such as:\n",
    "\n",
    "```\n",
    "#       B-Element\n",
    "1       I-Element\n",
    "Please  I-Element\n",
    ",       0\n",
    "...\n",
    "```\n",
    "\n",
    "When the tagging of the first two words are ignored from further porcessing the tag of the third word still \"pollutes\" the data. \n",
    "\n",
    "Another idea is to use some similarity-based matching algorithm to identify (exclude) sentences which have not \"survived\" Open Sesame. However, there is another issue: Open sesmae produces UNK for \"the\" suggesting which effects sentence matching. Also this fact indicates that thee is somthing wrong with the pre-trained models I use for Open Seseame. Probable action: train Open Sesamae for pre-trained models myself. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How many sentnces come out from Open Sesame?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_file = \"../data/fsl_scratch.conll\" \n",
    "\n",
    "unique_sentences = []\n",
    "with open(my_file, \"r\") as output_from_sesame:\n",
    "    sentence = []\n",
    "    for line in output_from_sesame:\n",
    "        line = line.strip(\"\\n\")\n",
    "        line = line.split(\"\\t\")\n",
    "        if line == [\"\"]:\n",
    "            if \" \".join(sentence) not in unique_sentences:\n",
    "                unique_sentences.append(\" \".join(sentence))\n",
    "            sentence = []\n",
    "        else:\n",
    "            #print(line)\n",
    "            sentence.append(line[1])\n",
    "print(len(unique_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ... and in the numbered version?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numbered version\n",
    "my_file = \"../data/fsl_scratch_numbered.conll\" \n",
    "\n",
    "unique_sentences = []\n",
    "with open(my_file, \"r\") as output_from_sesame:\n",
    "    sentence = []\n",
    "    for line in output_from_sesame:\n",
    "        line = line.strip(\"\\n\")\n",
    "        line = line.split(\"\\t\")\n",
    "        if line == [\"\"]:\n",
    "            if \" \".join(sentence) not in unique_sentences:\n",
    "                unique_sentences.append(\" \".join(sentence))\n",
    "            sentence = []\n",
    "        else:\n",
    "            #print(line)\n",
    "            sentence.append(line[1])\n",
    "print(len(unique_sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How does it look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unique_sentences[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in unique_sentences[:20]:\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Original count (the number of sentnces that goes into Open Sesame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/src_scratch.txt\") as f:\n",
    "    print(len(f.readlines()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Does any identitiy marker disapear? (e.g. --> \"UNK\") If no output: \"no\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "numbers = [f\"#{n}\" for n in range(1, 2001)]\n",
    "\n",
    "for x in unique_sentences:\n",
    "    x = x.split()\n",
    "    id_n = x[0]+x[1]\n",
    "    if id_n not in numbers:\n",
    "        print(id_n)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-alignment based on numbered sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dir_in  = \"../data/\"\n",
    "dir_out = \"../data/realigned/\"\n",
    "\n",
    "# Existing files:\n",
    "x_src  = \"src_scratch_forFSL.txt\" # this is the 2000 lines file with numbers\n",
    "XX_src = \"src_scratch.txt\"        # this is the 2000 lines file without numbers\n",
    "x_algn = \"alignments_scratch.txt\"\n",
    "x_grmr = \"trg_dep_scratch.txt\"\n",
    "x_ids  = \"ids_scratch.txt\"\n",
    "x_trg  = \"trg_scratch.txt\"\n",
    "\n",
    "# New files\n",
    "suffix = \"_realigned.txt\"\n",
    "new_trg  = dir_out + \"trg_scratch\" + suffix\n",
    "new_src  = dir_out + \"src_scratch\" + suffix\n",
    "new_algn = dir_out + x_algn[:-4] + suffix\n",
    "new_grmr = dir_out + x_grmr[:-4] + suffix\n",
    "new_ids  = dir_out + x_ids[:-4] + suffix\n",
    "\n",
    "my_iter = iter(unique_sentences) # these are the unique sentences out from Open Sesame - defined above\n",
    "#print(len(unique_sentences))\n",
    "LENGTH = 2000\n",
    "MAX = len(unique_sentences)\n",
    "missed = []\n",
    "i = 0\n",
    "\n",
    "with open(dir_in + x_src, \"r\") as xs, open(dir_in + XX_src, \"r\") as XXS, open(dir_in + x_algn, \"r\") as xa, open(dir_in + x_grmr, \"r\") as xg, open(dir_in + x_ids, \"r\") as xi, open(dir_in + x_trg, \"r\") as xt, open(new_trg, \"w\") as new_t, open(new_src, \"w\") as new_s, open(new_algn, \"w\") as new_a, open(new_grmr, \"w\") as new_g, open(new_ids, \"w\") as new_i:\n",
    "    while i < MAX:\n",
    "        #print(i, end=\"\\r\")\n",
    "        alignment  = xa.readline()\n",
    "        grammar    = xg.readline()\n",
    "        europ_id   = xi.readline()\n",
    "        target     = xt.readline()\n",
    "        source     = XXS.readline()\n",
    "        from_fsl   = next(my_iter)\n",
    "        from_fsl   = from_fsl.split()\n",
    "        idn_after  = from_fsl[1]\n",
    "        src_before = xs.readline().split()\n",
    "        #print(src_before)\n",
    "        idn_before = src_before[0][1:]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Here comes problem. Since Open Sesame tags the id \"# 34\",\n",
    "        # this will f-ck up the statement \"if idn_after == idn_before:\".\n",
    "        # In the tagged cases, the statement return False, when we want\n",
    "        # True.\n",
    "        \n",
    "        if idn_after == idn_before: # compare unique_sentences with original source (numbered)\n",
    "            \n",
    "            new_t.write(target) # here we could potentially remove the id# if we want to\n",
    "            new_s.write(source)\n",
    "            new_a.write(alignment)\n",
    "            new_g.write(grammar)\n",
    "            new_i.write(europ_id)\n",
    "        else:\n",
    "            #print(idn_after, idn_before)\n",
    "            while idn_after != idn_before:\n",
    "                missed.append(idn_before)\n",
    "                #print(len(missed), end=\"\\r\")\n",
    "                alignment  = xa.readline()\n",
    "                grammar    = xg.readline()\n",
    "                europ_id   = xi.readline()\n",
    "                target     = xt.readline()\n",
    "                source     = XXS.readline()\n",
    "                src_before = xs.readline().split()\n",
    "                idn_before = src_before[0][1:]\n",
    "            \n",
    "            #print(idn_after, idn_before)\n",
    "            new_t.write(target) # here we could potentially remove the id# if we want to\n",
    "            new_s.write(source)\n",
    "            new_a.write(alignment)\n",
    "            new_g.write(grammar)\n",
    "            new_i.write(europ_id)\n",
    "        \n",
    "        i += 1\n",
    "\n",
    "print(len(missed))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "missed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-alignment based on similiarity ... TBC"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "LENGTH = 2000\n",
    "from sentence_similarity import sentence_similarity\n",
    "#https://pypi.org/project/sentence-similarity/\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "log = []\n",
    "\n",
    "model=sentence_similarity(model_name='distilbert-base-uncased',embedding_type='cls_token_embedding')\n",
    "\n",
    "scores = []\n",
    "my_iter = iter(unique_sentences)\n",
    "\n",
    "with open(\"../data/src_scratch.txt\") as original:\n",
    "    for i in range(len(unique_sentences)):\n",
    "        o = original.readline().strip(\"\\n\")\n",
    "        s = next(my_iter)\n",
    "\n",
    "        s = s.replace(\"UNK\", \"the\")\n",
    "        \n",
    "        score = model.get_score(o, s,metric=\"cosine\")\n",
    "\n",
    "        if len(o.split()) != len(s.split()):\n",
    "            error=0\n",
    "        \n",
    "            while score < 0.80:\n",
    "                log.append(o)\n",
    "\n",
    "                o = original.readline().strip(\"\\n\")\n",
    "                score = model.get_score(o, s,metric=\"cosine\")\n",
    "                error+=1\n",
    "                if error > 20:\n",
    "                    print(\"Something went wrong!\")\n",
    "                    break\n",
    "        \n",
    "        #print(score, o, \"###\", s)\n",
    "        scores.append(score)\n",
    "        print(len(scores), len(log), \" \"*10, end=\"\\r\")\n",
    "            \n",
    "#print(len(scores))    \n",
    "plt.plot(scores)\n",
    "plt.show()  \n",
    "\n",
    "for s in log:\n",
    "    print(s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplify so called conll format to one sentence's annotation per line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**A problem -- Note to Self**\n",
    "*(4 Jan) I think realignment work, but simplification does not, for some reason. There is no 1:1 mapping between sentences and their annotations. Why?*\n",
    "Here is the problem to be solved:\n",
    "1. collected \"Resumption of the session\"\n",
    "2. comes to \"\\n\" --> equal to previous? No, so write to file and reset. Set `previous` to \"Resumption of ...\"\n",
    "3. collect \"Please rise ...\"\n",
    "4. comes to \"\\n\" --> **equal to previous? No** so again write to file and reset ...\n",
    "\n",
    "... now, after the next collection: `previous == sent`, the list will be popoulated, but the first frame annotation is already printed to file. \n",
    "\n",
    "**A decision can only be made when there are two sentences collected**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\"\n",
    "#my_conll_annot = data_dir + \"fsl_scratch.conll\"\n",
    "#simplified_fsl = data_dir + \"pp_fsl_scratch.txt\"\n",
    "my_conll_annot = data_dir + \"fsl_scratch_numbered.conll\" \n",
    "simplified_fsl = data_dir + \"pp_fsl_scratch_numbered.txt\"\n",
    "check_up_file  = data_dir + \"conll-pp-check_up.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** indexing must be the same in transfer. Awsome align start from 0. Open-Sesame output start from 1. My numbering of lines adds 2 more to this. Eurparl annotations for dep rel also strat from 1, but these are only references. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_conll(raw, initializer = 0, sep = \";\", adjustment = 2):\n",
    "    \"\"\" \n",
    "    :param adjustment: compensates (substracts) word indicies for, e.g., numbering \"#1 text\"\n",
    "                       adjustment = 2 adjusts for \"# 1 text\"\n",
    "    \"\"\"\n",
    "    \n",
    "    sentence = []\n",
    "    annotation = []\n",
    "    fes = []\n",
    "    fe = None\n",
    "    \n",
    "    for i, line in enumerate(raw.split(\"\\n\")[adjustment:-1], start = initializer):\n",
    "        #i = i - adjustment\n",
    "        \n",
    "        feat = line.split(\"\\t\")\n",
    "\n",
    "        sentence.append(feat[1]) # word\n",
    "        \n",
    "        if feat[-3] != \"_\": # LU\n",
    "            lu_idx = str(i)\n",
    "            frame = feat[-2] # frame\n",
    "        \n",
    "        fe_col = feat[-1]\n",
    "            \n",
    "        if fe_col == \"_\":\n",
    "            if fe != None: # FE\n",
    "                fes.append(f\"{start}:{end+1}#{fe}\")\n",
    "                fe = None\n",
    "                continue\n",
    "        else:\n",
    "            if fe_col[0] == \"S\":\n",
    "                fes.append(f\"{i}:{i+1}#{fe_col[2:]}\")\n",
    "                continue\n",
    "            if fe_col[0] == \"B\":\n",
    "                start = i\n",
    "                fe = fe_col[2:]\n",
    "                continue\n",
    "            if fe_col[0] == \"I\":\n",
    "                end = i\n",
    "    \n",
    "    festr = \"|\".join(fes)\n",
    "    annotation = f\"{frame};{lu_idx};{festr}\"\n",
    "    \n",
    "    return sentence, annotation    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terminator(string):\n",
    "    \n",
    "    out = []\n",
    "    \n",
    "    #for i, x in enumerate(string.split(\"\\n\")):\n",
    "    #    print(i,x)\n",
    "    \n",
    "    for line in string.split(\"\\n\")[:-1]:\n",
    "\n",
    "        line = line.split(\"\\t\")\n",
    "        suffix = \"-\".join(line[-3:])\n",
    "        if suffix == \"_-_-O\":\n",
    "            line = line[1]\n",
    "        else:\n",
    "            line = line[1] + \" [\" + suffix + \"]\"\n",
    "        out.append(line)\n",
    "    \n",
    "    out = \" \".join(out)\n",
    "    #print(out)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(conll_file, output_file, check_file, sep=\" \"):\n",
    "    \n",
    "    with open(conll_file, \"r\") as original, open(output_file, \"w\") as output, open(check_file, \"w\") as check:\n",
    "        \n",
    "        raw        = \"\"\n",
    "        annot_sent = []\n",
    "        conll_in   = []\n",
    "        previous   = None\n",
    "        buffer     = False\n",
    "        \n",
    "        for line in original:\n",
    "            if line == \"\\n\":\n",
    "                sent, annot_lu = read_conll(raw)\n",
    "                conll          = terminator(raw)\n",
    "\n",
    "                if sent == previous:\n",
    "                    annot_sent.append(annot_lu)\n",
    "                    conll_in.append(conll)\n",
    "                    raw = \"\"\n",
    "                    previous = sent\n",
    "                    buffer = True\n",
    "                    continue\n",
    "                else:\n",
    "                    if buffer:\n",
    "                        output.write(sep.join(annot_sent) + \"\\n\")\n",
    "                        annot_sent = []                        \n",
    "                        annot_sent.append(annot_lu)\n",
    "                    \n",
    "                        check.write(\" << NEXT ONE >> \".join(conll_in) + \"\\n\")\n",
    "                        conll_in = []\n",
    "                        conll_in.append(conll)\n",
    "                    \n",
    "                        raw = \"\"\n",
    "                        previous = sent\n",
    "                        buffer = True\n",
    "                        continue\n",
    "                    else:\n",
    "                        annot_sent.append(annot_lu)\n",
    "                        conll_in.append(conll)\n",
    "                        raw = \"\"\n",
    "                        previous = sent\n",
    "                        buffer = True\n",
    "                        continue\n",
    "            raw += line\n",
    "        output.write(sep.join(annot_sent) + \"\\n\")              # to print whats left in the buffer\n",
    "        check.write(\" << NEXT ONE >> \".join(conll_in) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "simplify(my_conll_annot, simplified_fsl, check_up_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Transfer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE:** Keep track of word indices!\n",
    "1. Python start from 0\n",
    "2. Awsome start from 0\n",
    "3. Open S output start from 1\n",
    "4. Enumeration prior FSL --> \"# 23 text text\"\n",
    "\n",
    "**SOLUTION:** This is solved above by parameters `initalizer` and `adjustment` in `read_conll` function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Anoter issue on deprel --> constituents: is there need of additional code to fully cover \"dependents of dependents\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aligner(alignment_dict, idx):\n",
    "    if idx in alignment_dict:\n",
    "        return int(alignment_dict[idx])\n",
    "    else:\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_alignment(string):\n",
    "    d = {}\n",
    "    for ww in string.split():\n",
    "        d[ww.split(\"-\")[0]] = ww.split(\"-\")[1] # SRC --> TRG ?\n",
    "    # d = aligner(d)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_grammar(string):\n",
    "    \"\"\" For every idx, list dependents.\n",
    "    \"\"\"\n",
    "    \n",
    "    # there can be several ROOT\n",
    "    \n",
    "    drs = string.split()\n",
    "    #print(drs)\n",
    "    \n",
    "    ref2idx = {int(dr.split(\";\")[0]): i for i, dr in enumerate(drs)}\n",
    "    ref2idx[None] = None\n",
    "    #print(ref2idx)\n",
    "    \n",
    "    #head_of = {}\n",
    "    h2d  = {idx: [] for idx in [v for v in ref2idx.values()]}\n",
    "    #print(head_of)\n",
    "    \n",
    "    for dr in drs:\n",
    "        dr = dr.split(\";\")\n",
    "        ref  = ref2idx[int(dr[0])]\n",
    "        head = ref2idx[None if dr[1] == \"\" else int(dr[1])] # the ROOT has no head\n",
    "        \n",
    "        h2d[head].append([ref])\n",
    "    \n",
    "    #print(\"before\", h2d)\n",
    "        \n",
    "    # Phrases ...\n",
    "    for head in h2d:\n",
    "        if h2d[head] == []:\n",
    "            continue\n",
    "        constituents = h2d[head]\n",
    "        update = []\n",
    "        for constituent in constituents:\n",
    "            #print(\"c\", constituent)\n",
    "            for w in constituent:\n",
    "                new_constituents = h2d[w]\n",
    "                for nc in new_constituents:\n",
    "                    for nw in nc:\n",
    "                        if nw not in constituent:\n",
    "                            #print(constituent)\n",
    "                            constituent += [nw]\n",
    "            update.append(sorted(constituent))\n",
    "        h2d[head] = sorted(update)\n",
    "    \n",
    "    return h2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "sentence = \"Ni lät mig aldrig komma till tals .\".split()\n",
    "print(len(sentence))\n",
    "print(\"-\"*10)\n",
    "grammar = read_grammar(\"1;5;SS 2;;ROOT 3;2;SS 4;2;TA 5;2;OO 6;5;OA 7;6;PA 8;2;IP\")\n",
    "print(grammar)\n",
    "#grammar[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N_align(start, end, constituent, adict):\n",
    "    count = 0\n",
    "    for idx in range(start, end):\n",
    "        if aligner(adict, idx) in constituent:\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transfer(fsl, alignment, deprel, ids, output_file, log_file, algorithm = \"Tonelli2\", wasteful = True):\n",
    "    \"\"\" \n",
    "    :param fsl: a simplified FSL file\n",
    "    :param alignment:\n",
    "    :param deprel:\n",
    "    :param ids:\n",
    "    :param output_file:\n",
    "    :param log_file:\n",
    "    :param algorithm: \n",
    "    :param wasteful:\n",
    "    \"\"\"\n",
    "    \n",
    "    whereiam = 1\n",
    "    \n",
    "    with open(fsl, \"r\") as FSL, open(alignment, \"r\") as A, open(deprel, \"r\") as G, open(ids, \"r\") as ID, open(output_file, \"w\") as O, open(log_file, \"w\") as LOG:\n",
    "        for line in FSL:\n",
    "            \n",
    "            line = line.strip(\"\\n\")\n",
    "            #print(whereiam, line)\n",
    "            idn = ID.readline().strip(\"\\n\")\n",
    "            dep_rel = G.readline().strip(\"\\n\")\n",
    "            adict = read_alignment(A.readline().strip(\"\\n\"))\n",
    "            #print(whereiam, adict)\n",
    "            \n",
    "            log = []\n",
    "            t_annotations = []\n",
    "            \n",
    "            frames = line.split(\" \")    # \"Frame;idx;start:end#Role|start:end#Role Frame;idx;start:end#Role|start:end#Role\"\n",
    "                                        # there can be multiple frames per sentence\n",
    "            \n",
    "            for order, this_frame in enumerate(frames):\n",
    "                #trg_frame = [] # to populate\n",
    "                fs_info = this_frame.split(\";\") # Frame;index_LU;FEs\n",
    "                frame_name = fs_info[0]\n",
    "                trg_lu = aligner(adict, fs_info[1]) # index of LU in src --> index of LU in trg\n",
    "                if trg_lu == None:\n",
    "                    trg_lu = \"NoLUalign\"\n",
    "                    log.append(f\"No target alignment--fr{order}\")\n",
    "                    #O.write(\"\\n\")\n",
    "                    #continue # is there any point considering fes when there is no trg alignment?\n",
    "                \n",
    "                src_fes = []\n",
    "                trg_fes = []\n",
    "                \n",
    "                if fs_info[-1] == \"\": #i.e. if there are no frame elements\n",
    "                    trg_fes_str = \"NoFEsinSRC\"\n",
    "                    trg_frame = f\"{frame_name};{trg_lu};{trg_fes_str}\"\n",
    "                    t_annotations.append(trg_frame)\n",
    "                else:\n",
    "                    if trg_lu == \"NoLUalign\":\n",
    "                        trg_fes_str = \"FEsinSRCbutNoLU\"\n",
    "                        trg_frame = f\"{frame_name};{trg_lu};{trg_fes_str}\"\n",
    "                        t_annotations.append(trg_frame)\n",
    "                    else:\n",
    "                        for x in fs_info[-1].split(\"|\"): # pipe(|) separates FEs within frame\n",
    "                            x = x.split(\"#\")             # hashtag (#) separates span from FE name\n",
    "                            y = x[0].split(\":\")          # colon (:) separates start from stop in span\n",
    "                            y = [int(v) for v in y]      # make start and stop integers\n",
    "                            z = tuple(y + [x[-1]])       # (start, stop, FE_name)\n",
    "                            src_fes.append(z)\n",
    "\n",
    "                        if algorithm == \"Yang\":\n",
    "                            pass\n",
    "                            # do something\n",
    "                            # Note: must avoid (a) discontinous and (b) overlapping elements in trg\n",
    "\n",
    "                        if algorithm == \"Tonelli2\":\n",
    "                            grammar = read_grammar(dep_rel)\n",
    "                            if trg_lu not in grammar:\n",
    "                                trg_fes_str = \"NoDepTRGLU\"\n",
    "                            else:\n",
    "                                lu_trg_dependents = grammar[trg_lu]\n",
    "\n",
    "                                for start, end, fe_name in src_fes:\n",
    "                                    best_score = 0\n",
    "                                    best_candidate = None\n",
    "\n",
    "                                    for dp in lu_trg_dependents: # every dependent should be a list of word indices\n",
    "                                        score = N_align(start, end, dp, adict)\n",
    "                                        if score > best_score:   # what if the same score?\n",
    "                                            best_score = score\n",
    "                                            best_candidate = dp  # dp is a list of numbers word indices\n",
    "\n",
    "                                if best_score != 0:\n",
    "                                    start = min(best_candidate)  # note again structure of dp: a list of numbers\n",
    "                                    end   = max(best_candidate)\n",
    "                                    # Note assumption here:\n",
    "                                    # Dependents are assumed to be continous,\n",
    "                                    # while in fact dependets as derived from\n",
    "                                    # read_grammar() are discontinious.\n",
    "                                    trg_fe = f\"{start}:{end}#{fe_name}\" # add 1 to end?\n",
    "                                    trg_fes.append(trg_fe)\n",
    "\n",
    "                                #if wasteful == True:\n",
    "                                #    if len(src_fes) != len(trg_fes):\n",
    "                                #        O.write(\"/n\")\n",
    "                                #        log.append(f\"Mismatch of frame elements\")\n",
    "                                #        continue\n",
    "\n",
    "                                if trg_fes == []:\n",
    "                                    trg_fes_str = \"NoTransfer\"\n",
    "                                else:\n",
    "                                    trg_fes_str = \"|\".join(trg_fes)\n",
    "                            \n",
    "                            trg_frame = f\"{frame_name};{trg_lu};{trg_fes_str}\"\n",
    "                            t_annotations.append(trg_frame)\n",
    "            \n",
    "            \n",
    "            \n",
    "            #print(whereiam, \" \".join(t_annotations))\n",
    "            whereiam += 1\n",
    "\n",
    "            O.write(\" \".join(t_annotations) + \"\\n\")\n",
    "                            \n",
    "            if log != []:\n",
    "                LOG.write(f\"{idn}:\" + \";\".join(log) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result  = \"../data/results/result.txt\"\n",
    "logfile = \"../data/results/log.txt\"\n",
    "\n",
    "fs_annotations = \"../data/pp_fsl_scratch_numbered.txt\"\n",
    "alignments     = \"../data/realigned/alignments_scratch_realigned.txt\"\n",
    "grammar        = \"../data/realigned/trg_dep_scratch_realigned.txt\"\n",
    "ids            = \"../data/realigned/ids_scratch_realigned.txt\"\n",
    "\n",
    "transfer(fsl=fs_annotations, \n",
    "         alignment=alignments, \n",
    "         deprel=grammar, \n",
    "         ids=ids, \n",
    "         output_file=result, \n",
    "         log_file=logfile, \n",
    "         algorithm = \"Tonelli2\", \n",
    "         wasteful = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle, seed\n",
    "seed(1)\n",
    "\n",
    "def inspector(source, target, alignment, frame_semantics, grammar, conll2pp, sample = None):\n",
    "    \n",
    "    with open(source, \"r\") as f:\n",
    "        SRC = f.readlines()\n",
    "        \n",
    "    with open(target, \"r\") as f:\n",
    "        TRG = f.readlines()\n",
    "        \n",
    "    with open(alignment, \"r\") as f:\n",
    "        ALG = f.readlines()\n",
    "    \n",
    "    with open(frame_semantics, \"r\") as f:\n",
    "        FSL = f.readlines()\n",
    "        \n",
    "    with open(grammar,\"r\") as f:\n",
    "        GRM = f.readlines()\n",
    "    \n",
    "    with open(conll2pp, \"r\") as f:\n",
    "        C2P = f.readlines()\n",
    "    \n",
    "    everything = list(zip(SRC, TRG, ALG, FSL, GRM, C2P))\n",
    "    shuffle(everything)\n",
    "    if sample != None:\n",
    "        sample = everything[:sample]\n",
    "    else:\n",
    "        sample = everything\n",
    "    myiter = iter(sample)\n",
    "    \n",
    "    return myiter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source    = \"../data/realigned/src_scratch_realigned.txt\"\n",
    "target    = \"../data/realigned/trg_scratch_realigned.txt\"\n",
    "alignment = \"../data/realigned/alignments_scratch_realigned.txt\"\n",
    "frame_sem = \"../data/pp_fsl_scratch_numbered.txt\"\n",
    "grammar   = \"../data/realigned/trg_dep_scratch_realigned.txt\"\n",
    "conll2pp  = \"../data/conll-pp-check_up.txt\"\n",
    "\n",
    "my_data = inspector(source, target, alignment, frame_sem, grammar, conll2pp, sample = 10)\n",
    "\n",
    "for x in my_data:\n",
    "    for y in x:\n",
    "        y = y.strip(\"\\n\")\n",
    "        print(len(y.split()), \"\\t\", y.replace(\" << NEXT ONE >> \", \"\\n\\t\"))\n",
    "    print(\"-\"*20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(file1, file2, out, span):\n",
    "    with open(file1, \"r\") as f:\n",
    "        one = f.readlines()\n",
    "    with open(file2, \"r\") as f:\n",
    "        two = f.readlines()\n",
    "        \n",
    "    with open(out, \"w\") as f:\n",
    "        for one, two in zip(one, two):\n",
    "            one = one.strip(\"\\n\")\n",
    "            two = two.strip(\"\\n\")\n",
    "            f.write(f\"{one[:span]} <<>> {two[:span]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare(\"../data/realigned/src_scratch_realigned.txt\", \"../data/conll-pp-check_up.txt\", \"../data/find_it.txt\", 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "def src_evaluate(fsl_src, out):\n",
    "    \n",
    "    frames = {}\n",
    "    frames_per_sent = {}\n",
    "    fes_per_frame = []\n",
    "    f_list = []\n",
    "    missing_fe = 0\n",
    "    lu_being_fe = {}\n",
    "    \n",
    "    i=0\n",
    "   \n",
    "    with open(fsl_src, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            frames_sent = set()\n",
    "            for frame in line.split():\n",
    "                frame = frame.split(\";\")\n",
    "                name = frame[0]\n",
    "                frames_sent.add(name)\n",
    "                f_list.append(name)\n",
    "                if name in frames:\n",
    "                    frames[name] += 1\n",
    "                else:\n",
    "                    frames[name] = 1\n",
    "                lu = int(frame[1])\n",
    "                fes  = frame[2]\n",
    "                if fes == \"\":\n",
    "                    missing_fe += 1\n",
    "                    fes_per_frame.append(0)\n",
    "                else:\n",
    "                    fes_lst = fes.split(\"|\")\n",
    "                    fes_per_frame.append(len(fes_lst))\n",
    "                    for fe in fes_lst:\n",
    "                        fe = fe.split(\"#\")\n",
    "                        start, stop = tuple([int(x) for x in fe[0].split(\":\")])\n",
    "                        fe_typ = fe[1]\n",
    "                        if start == lu:\n",
    "                            if stop == start + 1:\n",
    "                                if fe_typ in lu_being_fe:\n",
    "                                    lu_being_fe[fe_typ] += 1\n",
    "                                else: \n",
    "                                    lu_being_fe[fe_typ] = 1\n",
    "            for fsx in frames_sent:\n",
    "                if fsx in frames_per_sent:\n",
    "                    frames_per_sent[fsx] += 1\n",
    "                else:\n",
    "                    frames_per_sent[fsx] = 1\n",
    "                \n",
    "    \n",
    "    length = len(lines)\n",
    "    topk = 15\n",
    "    frames_count = list(frames.items())\n",
    "    frames_count.sort(key=itemgetter(1), reverse=True)\n",
    "    \n",
    "    print(f\"Length: {length}\")\n",
    "    print(f\"No. of frames (types): {len(frames.keys())}\")\n",
    "    print(f\"No. of frame tokens: {len(f_list)}\")\n",
    "    print(f\"Frames missing FEs: {missing_fe}\")\n",
    "    print(f\"Frame per sentence: {round(len(f_list)/length, 3)}\")\n",
    "    print(f\"Frame elements per frame: {round(sum(fes_per_frame)/len(fes_per_frame), 3)}\")\n",
    "    exm = [x for x in fes_per_frame if x > 0]\n",
    "    print(f\"Frame elements per frame (excl. missing): {round(sum(exm)/len(exm), 3)}\")\n",
    "    print(f\"No. of frames where LU is also FE: {sum(lu_being_fe.values())}\")\n",
    "\n",
    "    print()\n",
    "    print(f\"Top {topk} frames:\")\n",
    "    for f in frames_count[:topk]:\n",
    "        print(f\"{f[0]}\\t{f[1]}\\t{frames_per_sent[f[0]]}\\t{round((frames_per_sent[f[0]]/length)*100, 1)}\")\n",
    "        #print(f\"{f[0]}\\t{f[1]}\\t{round((f[1]/length)*100, 1)}\")\n",
    "\n",
    "def transfer_eval(transfer_file, trg_file, out_file):\n",
    "    \n",
    "    f_status  = {}\n",
    "    frames_per_sent = {}\n",
    "    no_lu = 0\n",
    "    frames = {}\n",
    "    \n",
    "    with open(transfer_file, \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            frames_sent = set()\n",
    "            for frame in line.split():\n",
    "                frame = frame.split(\";\")\n",
    "                name = frame[0]\n",
    "                frames_sent.add(name)\n",
    "                \n",
    "                lu = frame[1]\n",
    "                if lu == \"NoLUalign\":\n",
    "                    no_lu += 1\n",
    "                else:\n",
    "                    if name in frames:\n",
    "                        frames[name] += 1\n",
    "                    else:\n",
    "                        frames[name] = 1\n",
    "                \n",
    "                fe = frame[2]\n",
    "                if fe in f_status:\n",
    "                    f_status[fe] += 1\n",
    "                else:\n",
    "                    f_status[fe] = 1\n",
    "            for fsx in frames_sent:\n",
    "                if fsx in frames_per_sent:\n",
    "                    frames_per_sent[fsx] += 1\n",
    "                else:\n",
    "                    frames_per_sent[fsx] = 1\n",
    "    \n",
    "    length = len(lines)\n",
    "    frames_count = list(frames.items())\n",
    "    frames_count.sort(key=itemgetter(1), reverse=True)\n",
    "    topk = 15\n",
    "    \n",
    "    # 'NoFEsinSRC', 'NoTransfer', 'FEsinSRCbutNoLU', 'NoDepTRGLU'\n",
    "    for s in f_status.keys():\n",
    "        print(s, f_status[s])\n",
    "        \n",
    "    print(\"No LU Alignment:\", no_lu)\n",
    "    \n",
    "    print(f\"Top {topk} frames:\")\n",
    "    for f in frames_count[:topk]:\n",
    "        print(f\"{f[0]}\\t{f[1]}\\t{frames_per_sent[f[0]]}\\t{round((frames_per_sent[f[0]]/length)*100, 1)}\")\n",
    "        \n",
    "                    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length: 1968\n",
      "No. of frames (types): 446\n",
      "No. of frame tokens: 12071\n",
      "Frames missing FEs: 6489\n",
      "Frame per sentence: 6.134\n",
      "Frame elements per frame: 0.592\n",
      "Frame elements per frame (excl. missing): 1.281\n",
      "No. of frames where LU is also FE: 1958\n",
      "\n",
      "Top 15 frames:\n",
      "Origin\t1986\t1126\t57.2\n",
      "Degree\t1024\t772\t39.2\n",
      "Causation\t563\t485\t24.6\n",
      "Means\t518\t415\t21.1\n",
      "Taking_time\t292\t263\t13.4\n",
      "Possession\t279\t251\t12.8\n",
      "Negation\t262\t247\t12.6\n",
      "Performers_and_roles\t233\t224\t11.4\n",
      "Proportional_quantity\t202\t184\t9.3\n",
      "Being_employed\t161\t153\t7.8\n",
      "Quantified_mass\t160\t155\t7.9\n",
      "Measure_volume\t158\t151\t7.7\n",
      "Type\t157\t146\t7.4\n",
      "Likelihood\t156\t148\t7.5\n",
      "Organization\t138\t129\t6.6\n"
     ]
    }
   ],
   "source": [
    "src_evaluate(\"../data/pp_fsl_scratch_numbered.txt\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'NoFEsinSRC'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-84-5d36a5eca62d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msrc_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../data/results/result.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-83-36879c4fe7d6>\u001b[0m in \u001b[0;36msrc_evaluate\u001b[0;34m(fsl_src, out)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mfe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfes_lst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                         \u001b[0mfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                         \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                         \u001b[0mfe_typ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-83-36879c4fe7d6>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     34\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mfe\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfes_lst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                         \u001b[0mfe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m                         \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\":\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m                         \u001b[0mfe_typ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfe\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mstart\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlu\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'NoFEsinSRC'"
     ]
    }
   ],
   "source": [
    "src_evaluate(\"../data/results/result.txt\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NoFEsinSRC 6489\n",
      "NoTransfer 4344\n",
      "FEsinSRCbutNoLU 1188\n",
      "NoDepTRGLU 50\n",
      "No LU Alignment: 2832\n",
      "Top 15 frames:\n",
      "Origin\t1218\t1126\t57.2\n",
      "Degree\t867\t772\t39.2\n",
      "Causation\t434\t485\t24.6\n",
      "Means\t385\t415\t21.1\n",
      "Taking_time\t238\t263\t13.4\n",
      "Negation\t232\t247\t12.6\n",
      "Possession\t191\t251\t12.8\n",
      "Proportional_quantity\t157\t184\t9.3\n",
      "Performers_and_roles\t150\t224\t11.4\n",
      "Measure_volume\t138\t151\t7.7\n",
      "Likelihood\t133\t148\t7.5\n",
      "Quantified_mass\t129\t155\t7.9\n",
      "Organization\t126\t129\t6.6\n",
      "Being_employed\t108\t153\t7.8\n",
      "Time_vector\t108\t126\t6.4\n"
     ]
    }
   ],
   "source": [
    "transfer_eval(\"../data/results/result.txt\", None, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def xml_annotator(trg, transfer):\n",
    "    pass\n",
    "       \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obsolete ... "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def oracle(to_evaluate, criterion = []):\n",
    "    \n",
    "    proceed = False\n",
    "    \n",
    "    for element in to_evaluate:\n",
    "        if element != criterion:\n",
    "            proceed = True\n",
    "            break\n",
    "    \n",
    "    return proceed"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def xtender(dictionary, root=None):\n",
    "    \n",
    "    next_up = dictionary[root]\n",
    "    print(\"nu\", next_up)\n",
    "    \n",
    "    while oracle(next_up):\n",
    "        #print(\"xxx\")\n",
    "        constituents = next_up\n",
    "        next_up = []\n",
    "        for constituent in constituents:\n",
    "            print(\"c\", constituent)\n",
    "            for w in constituent:\n",
    "                print(\"w\", w)\n",
    "                print(\"dw\", dictionary[w])\n",
    "                constituent += dictionary[w]\n",
    "                print(\"c\", constituents)\n",
    "                next_up.extend(dictionary[w])\n",
    "                print(\"nu\", next_up)\n",
    "                \n",
    "    \n",
    "    return dictionary\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "xtender(grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplify(conll_file, output_file, check_file, sep=\" \"):\n",
    "    \n",
    "    with open(conll_file, \"r\") as original, open(output_file, \"w\") as output, open(check_file, \"w\") as check:\n",
    "        \n",
    "        raw = \"\"\n",
    "        annot_sent = []\n",
    "        conll_in   = []\n",
    "        previous   = None\n",
    "        \n",
    "        for line in original:\n",
    "            if line == \"\\n\":\n",
    "                sent, annot_lu = read_conll(raw)\n",
    "                conll          = terminator(raw)\n",
    "                \n",
    "                \n",
    "                \n",
    "                if sent == previous:\n",
    "                    annot_sent.append(annot_lu)\n",
    "                    conll_in.append(conll)\n",
    "                    raw = \"\"\n",
    "                    previous = sent\n",
    "                    continue\n",
    "                \n",
    "                else:\n",
    "                    \n",
    "                    \n",
    "                    annot_sent.append(annot_lu)\n",
    "                    output.write(sep.join(annot_sent) + \"\\n\")\n",
    "                    annot_sent = []\n",
    "                    conll_in.append(conll)\n",
    "                    check.write(\" << NEXT ONE >> \".join(conll_in) + \"\\n\")\n",
    "                    conll_in = []\n",
    "                    raw = \"\"\n",
    "                    previous = sent\n",
    "                    continue\n",
    "            raw += line\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
